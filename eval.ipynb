{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "\n",
    "from src.preprocessing_mouse_GSE115746 import (\n",
    "    cell_cluster_cell_type_to_spot_composition,\n",
    "    cell_subclass_to_spot_composition,\n",
    ")\n",
    "from src.preprocessing_spotless import get_st_sub_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "dset_dir = \"preprocessed_data/dlpfc/all/raw_counts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dset_dir,\"st_sample_id_l.pkl\"), \"rb\") as f:\n",
    "    st_sample_id_l = pickle.load(f)\n",
    "\n",
    "mat_sp_samp_split_d = sc.read_h5ad(os.path.join(dset_dir, \"unscaled/mat_sp_samp_split_d.h5ad\"))\n",
    "st_splits = mat_sp_samp_split_d.obs['split'].unique().tolist()\n",
    "\n",
    "with open(os.path.join(dset_dir,\"sc_sub_dict2.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(dset_dir,\"sc_sub_dict.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict = pickle.load(f)\n",
    "\n",
    "st_sample_id_d = OrderedDict()\n",
    "for split in splits:\n",
    "    sids = set(mat_sp_samp_split_d.obs.loc[:,\"sample_id\"][mat_sp_samp_split_d.obs.loc[:,\"split\"] == split])\n",
    "    if split not in st_splits:\n",
    "        continue\n",
    "    st_sample_id_d[split] = [\n",
    "        sid for sid in st_sample_id_l if sid in sids\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(os.path.join(dset_dir, \"st_predictions.h5\"), \"r\") as f:\n",
    "\n",
    "    predictions = f[\"predictions\"][()]\n",
    "\n",
    "col_names = pd.read_csv(\n",
    "    os.path.join(dset_dir, \"pred_columns.csv\"), header=None, index_col=None\n",
    ")[0]\n",
    "row_names = pd.read_csv(\n",
    "    os.path.join(dset_dir, \"pred_rows.csv\"), header=None, index_col=None\n",
    ")[0]\n",
    "\n",
    "predictions = pd.DataFrame(predictions, index=row_names, columns=col_names)\n",
    "\n",
    "predictions.index = predictions.index.str.replace(\".\", \"-\", regex=False)\n",
    "predictions.columns = predictions.columns.str.replace(\"Slash\", \"/\", regex=False)\n",
    "\n",
    "cell_types = [sc_sub_dict[i] for i in range(len(sc_sub_dict))]\n",
    "predictions = predictions.reindex(columns=cell_types, fill_value=0.0)\n",
    "predictions = predictions.reindex(index=mat_sp_samp_split_d.obs.index, fill_value=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sp_d = OrderedDict()\n",
    "\n",
    "for split in splits:\n",
    "    split_sids = st_sample_id_d[split]\n",
    "    for split_sid in split_sids:\n",
    "        spots = mat_sp_samp_split_d.obs.index[mat_sp_samp_split_d.obs['sample_id'] == split_sid]\n",
    "\n",
    "        pred_sp_d[split_sid] = predictions.loc[spots].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ex_to_L_d = {\n",
    "    1: {5, 6},\n",
    "    2: {5},\n",
    "    3: {4, 5},\n",
    "    4: {6},\n",
    "    5: {5},\n",
    "    6: {4, 5, 6},\n",
    "    7: {4, 5, 6},\n",
    "    8: {5, 6},\n",
    "    9: {5, 6},\n",
    "    10: {2, 3, 4},\n",
    "}\n",
    "\n",
    "def _plot_roc(\n",
    "    visnum,\n",
    "    adata,\n",
    "    pred_sp,\n",
    "    num_name_exN_l,\n",
    "    numlist,\n",
    "):\n",
    "    \"\"\"Plot ROC for a given visnum\"\"\"\n",
    "\n",
    "    Ex_l = [t[2] for t in num_name_exN_l]\n",
    "    num_to_ex_d = dict(zip(numlist, Ex_l))\n",
    "\n",
    "    def layer_to_layer_number(x):\n",
    "        \"\"\"Converts a string of layers to a list of layer numbers\"\"\"\n",
    "        for char in x:\n",
    "            if char.isdigit():\n",
    "                # if in (ordinal -> ex number -> layers)\n",
    "                if int(char) in Ex_to_L_d[num_to_ex_d[visnum]]:\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "    y_pred = pred_sp[:, visnum]\n",
    "\n",
    "    # set unpredicted to 0\n",
    "    # This means to include unpredicted spots, right side (FPR=1) of ROC will \n",
    "    # represent threshold of 0; TPR will not include unpredicted spots until\n",
    "    # that point, removing proportion of unpredicted from the AUROC.\n",
    "    y_pred[~np.isfinite(y_pred)] = 0.0\n",
    "    y_true = adata.obs[\"spatialLIBD\"].map(layer_to_layer_number).fillna(0)\n",
    "\n",
    "    if y_true.sum() > 0:\n",
    "\n",
    "\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_name_exN_l = []\n",
    "for k, v in sc_sub_dict.items():\n",
    "    if \"Ex\" in v:\n",
    "        # (clust_ordinal, clust_name, Ex_clust_num)\n",
    "        num_name_exN_l.append((k, v, int(v.split(\"_\")[1])))\n",
    "\n",
    "num_name_exN_l.sort(key=lambda a: a[2])\n",
    "\n",
    "numlist = [t[0] for t in num_name_exN_l]  # clust ordinals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spot_scores = OrderedDict()\n",
    "\n",
    "for split in splits:\n",
    "    split_sids = st_sample_id_d[split]\n",
    "    for sample_id in split_sids:\n",
    "        # spot_scores[split] = OrderedDict()\n",
    "        da_aucs = []\n",
    "        for i, num in enumerate(numlist):\n",
    "            ax_ = None\n",
    "            da_aucs.append(\n",
    "                _plot_roc(\n",
    "                    num,\n",
    "                    mat_sp_samp_split_d[mat_sp_samp_split_d.obs[\"sample_id\"] == sample_id],\n",
    "                    pred_sp_d[sample_id],\n",
    "                    num_name_exN_l,\n",
    "                    numlist,\n",
    "                )\n",
    "            )\n",
    "    \n",
    "        spot_scores[(split, sample_id)] = np.nanmean(da_aucs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_d = OrderedDict()\n",
    "\n",
    "scores_d[\"dlpfc\"] = pd.Series(spot_scores).groupby(level=0).mean().reindex(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('dlpfc',\n",
       "              train    0.553414\n",
       "              val      0.482188\n",
       "              test     0.497089\n",
       "              dtype: float64)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train']\n",
    "\n",
    "dset_dir = \"preprocessed_data/pdac/all/raw_counts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dset_dir,\"st_sample_id_l.pkl\"), \"rb\") as f:\n",
    "    st_sample_id_l = pickle.load(f)\n",
    "\n",
    "mat_sp_samp_split_d = sc.read_h5ad(os.path.join(dset_dir, \"unscaled/mat_sp_train_d.h5ad\"))\n",
    "st_splits = mat_sp_samp_split_d.obs['split'].unique().tolist()\n",
    "\n",
    "with open(os.path.join(dset_dir,\"sc_sub_dict2.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(dset_dir,\"sc_sub_dict.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict = pickle.load(f)\n",
    "\n",
    "st_sample_id_d = OrderedDict()\n",
    "for split in splits:\n",
    "    sids = set(mat_sp_samp_split_d.obs.loc[:,\"sample_id\"][mat_sp_samp_split_d.obs.loc[:,\"split\"] == split])\n",
    "    if split not in st_splits:\n",
    "        continue\n",
    "    st_sample_id_d[split] = [\n",
    "        sid for sid in st_sample_id_l if sid in sids\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(os.path.join(dset_dir, \"st_predictions.h5\"), \"r\") as f:\n",
    "\n",
    "    predictions = f[\"predictions\"][()]\n",
    "\n",
    "col_names = pd.read_csv(\n",
    "    os.path.join(dset_dir, \"pred_columns.csv\"), header=None, index_col=None\n",
    ")[0]\n",
    "row_names = pd.read_csv(\n",
    "    os.path.join(dset_dir, \"pred_rows.csv\"), header=None, index_col=None\n",
    ")[0]\n",
    "\n",
    "predictions = pd.DataFrame(predictions, index=row_names, columns=col_names)\n",
    "\n",
    "predictions.index = predictions.index.str.replace(\".\", \"-\", regex=False)\n",
    "predictions.columns = predictions.columns.str.replace(\"Slash\", \"/\", regex=False)\n",
    "\n",
    "cell_types = [sc_sub_dict[i] for i in range(len(sc_sub_dict))]\n",
    "predictions = predictions.reindex(columns=cell_types, fill_value=0.0)\n",
    "predictions = predictions.reindex(index=mat_sp_samp_split_d.obs.index, fill_value=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sp_d = OrderedDict()\n",
    "\n",
    "for split in splits:\n",
    "    split_sids = st_sample_id_d[split]\n",
    "    for split_sid in split_sids:\n",
    "        spots = mat_sp_samp_split_d.obs.index[mat_sp_samp_split_d.obs['sample_id'] == split_sid]\n",
    "\n",
    "        pred_sp_d[split_sid] = predictions.loc[spots].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_roc_pdac(visnum, adata, pred_sp, sc_to_st_celltype):\n",
    "    \"\"\"Plot ROC for a given visnum (PDAC)\"\"\"\n",
    "    try:\n",
    "        cell_name = sc_sub_dict[visnum]\n",
    "    except TypeError:\n",
    "        cell_name = \"Other\"\n",
    "\n",
    "    def st_sc_bin(cell_type):\n",
    "        return int(cell_type in sc_to_st_celltype.get(cell_name, set()))\n",
    "\n",
    "    y_pred = pred_sp[:, visnum].squeeze()\n",
    "    y_pred = np.nan_to_num(y_pred, nan=0.0)\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = y_pred.sum(axis=1)\n",
    "    y_true = adata.obs[\"cell_type\"].map(st_sc_bin).fillna(0)\n",
    "\n",
    "    if y_true.sum() > 0:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_to_st_celltype = {\n",
    "    # Peng et al., 2019: Taken together, these results show that\n",
    "    # type 2 ductal cells are the major source of malignant cells in\n",
    "    # PDACs.\n",
    "    \"Ductal cell type 2\": {\"Cancer region\"},\n",
    "    \"T cell\": {\"Cancer region\", \"Stroma\"},\n",
    "    \"Macrophage cell\": {\"Cancer region\", \"Stroma\"},\n",
    "    \"Fibroblast cell\": {\"Cancer region\", \"Stroma\"},\n",
    "    \"B cell\": {\"Cancer region\", \"Stroma\"},\n",
    "    \"Ductal cell type 1\": {\"Duct epithelium\"},\n",
    "    \"Endothelial cell\": {\"Interstitium\"},\n",
    "    \"Stellate cell\": {\"Stroma\", \"Pancreatic tissue\"},\n",
    "    \"Acinar cell\": {\"Pancreatic tissue\"},\n",
    "    \"Endocrine cell\": {\"Pancreatic tissue\"},\n",
    "}\n",
    "\n",
    "\n",
    "celltypes = list(sc_to_st_celltype.keys()) + [\"Other\"]\n",
    "n_celltypes = len(celltypes)\n",
    "n_rows = int(math.ceil(n_celltypes / 5))\n",
    "\n",
    "numlist = [sc_sub_dict2.get(t) for t in celltypes[:-1]]\n",
    "numlist.extend([v for k, v in sc_sub_dict2.items() if k not in celltypes[:-1]])\n",
    "\n",
    "\n",
    "\n",
    "spot_scores = OrderedDict()\n",
    "\n",
    "for split in splits:\n",
    "    split_sids = st_sample_id_d[split]\n",
    "    spot_scores[split] = OrderedDict()\n",
    "    for sample_id in split_sids:\n",
    "        da_aucs = []\n",
    "        for i, num in enumerate(numlist[:-1]):\n",
    "            da_aucs.append(\n",
    "                _plot_roc_pdac(\n",
    "                    num,\n",
    "                    mat_sp_samp_split_d[mat_sp_samp_split_d.obs[\"sample_id\"] == sample_id],\n",
    "                    pred_sp_d[sample_id],\n",
    "                    sc_to_st_celltype,\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        spot_scores[split][sample_id] =  np.nanmean(da_aucs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('train', OrderedDict([('pdac_a', 0.5), ('pdac_b', 0.5)]))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spot_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_d[\"pdac\"] = pd.concat({k: pd.Series(v) for k, v in spot_scores.items()}).groupby(level=0).mean() #.reindex(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    0.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_d[\"pdac\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "dset_dir = \"preprocessed_data/spotless/all/raw_counts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dset_dir,\"st_sample_id_l.pkl\"), \"rb\") as f:\n",
    "    st_sample_id_l = pickle.load(f)\n",
    "\n",
    "mat_sp_samp_split_d = sc.read_h5ad(os.path.join(dset_dir, \"unscaled/mat_sp_samp_split_d.h5ad\"))\n",
    "st_splits = mat_sp_samp_split_d.obs['split'].unique().tolist()\n",
    "\n",
    "with open(os.path.join(dset_dir,\"sc_sub_dict2.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(dset_dir,\"sc_sub_dict.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict = pickle.load(f)\n",
    "\n",
    "st_sample_id_d = OrderedDict()\n",
    "for split in splits:\n",
    "    sids = set(mat_sp_samp_split_d.obs.loc[:,\"sample_id\"][mat_sp_samp_split_d.obs.loc[:,\"split\"] == split])\n",
    "    if split not in st_splits:\n",
    "        continue\n",
    "    st_sample_id_d[split] = [\n",
    "        sid for sid in st_sample_id_l if sid in sids\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(os.path.join(dset_dir, \"st_predictions.h5\"), \"r\") as f:\n",
    "\n",
    "    predictions = f[\"predictions\"][()]\n",
    "\n",
    "col_names = pd.read_csv(\n",
    "    os.path.join(dset_dir, \"pred_columns.csv\"), header=None, index_col=None\n",
    ")[0]\n",
    "row_names = pd.read_csv(\n",
    "    os.path.join(dset_dir, \"pred_rows.csv\"), header=None, index_col=None\n",
    ")[0]\n",
    "\n",
    "predictions = pd.DataFrame(predictions, index=row_names, columns=col_names)\n",
    "\n",
    "predictions.index = predictions.index.str.replace(\".\", \"-\", regex=False)\n",
    "predictions.columns = predictions.columns.str.replace(\"Slash\", \"/\", regex=False)\n",
    "\n",
    "cell_types = [sc_sub_dict[i] for i in range(len(sc_sub_dict))]\n",
    "predictions = predictions.reindex(columns=cell_types, fill_value=0.0)\n",
    "predictions = predictions.reindex(index=mat_sp_samp_split_d.obs.index, fill_value=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Astro</th>\n",
       "      <th>Batch Grouping</th>\n",
       "      <th>CR</th>\n",
       "      <th>Doublet Astro Aqp4 Ex</th>\n",
       "      <th>Doublet Endo</th>\n",
       "      <th>Doublet VISp L5 NP and L6 CT</th>\n",
       "      <th>Endo</th>\n",
       "      <th>High Intronic</th>\n",
       "      <th>L2/3 IT</th>\n",
       "      <th>L4</th>\n",
       "      <th>...</th>\n",
       "      <th>NP</th>\n",
       "      <th>Oligo</th>\n",
       "      <th>Peri</th>\n",
       "      <th>Pvalb</th>\n",
       "      <th>SMC</th>\n",
       "      <th>Serpinf1</th>\n",
       "      <th>Sncg</th>\n",
       "      <th>Sst</th>\n",
       "      <th>VLMC</th>\n",
       "      <th>Vip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spot_1</th>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021691</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.046968</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>0.131708</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>0.045656</td>\n",
       "      <td>0.033060</td>\n",
       "      <td>0.041988</td>\n",
       "      <td>0.034747</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.016008</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>0.067503</td>\n",
       "      <td>0.054646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spot_2</th>\n",
       "      <td>0.088842</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034059</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.064074</td>\n",
       "      <td>0.180086</td>\n",
       "      <td>0.072099</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014626</td>\n",
       "      <td>0.053172</td>\n",
       "      <td>0.028659</td>\n",
       "      <td>0.018539</td>\n",
       "      <td>0.033710</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>0.081648</td>\n",
       "      <td>0.033538</td>\n",
       "      <td>0.068744</td>\n",
       "      <td>0.065875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spot_3</th>\n",
       "      <td>0.283756</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.066877</td>\n",
       "      <td>0.090290</td>\n",
       "      <td>0.011668</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003812</td>\n",
       "      <td>0.058927</td>\n",
       "      <td>0.028577</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.053665</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.005422</td>\n",
       "      <td>0.039513</td>\n",
       "      <td>0.066950</td>\n",
       "      <td>0.061182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spot_4</th>\n",
       "      <td>0.059405</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031219</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.197762</td>\n",
       "      <td>0.185305</td>\n",
       "      <td>0.048333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>0.053192</td>\n",
       "      <td>0.032850</td>\n",
       "      <td>0.010993</td>\n",
       "      <td>0.031830</td>\n",
       "      <td>0.022006</td>\n",
       "      <td>0.012809</td>\n",
       "      <td>0.033963</td>\n",
       "      <td>0.053180</td>\n",
       "      <td>0.043159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spot_5</th>\n",
       "      <td>0.060381</td>\n",
       "      <td>0.020950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023918</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.043542</td>\n",
       "      <td>0.221648</td>\n",
       "      <td>0.111834</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>0.054637</td>\n",
       "      <td>0.028244</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.011814</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.029430</td>\n",
       "      <td>0.063621</td>\n",
       "      <td>0.068352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spot_5-3</th>\n",
       "      <td>0.061157</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018658</td>\n",
       "      <td>0.015643</td>\n",
       "      <td>0.055937</td>\n",
       "      <td>0.238554</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017319</td>\n",
       "      <td>0.090831</td>\n",
       "      <td>0.043767</td>\n",
       "      <td>0.007020</td>\n",
       "      <td>0.030524</td>\n",
       "      <td>0.015671</td>\n",
       "      <td>0.011555</td>\n",
       "      <td>0.023536</td>\n",
       "      <td>0.059534</td>\n",
       "      <td>0.023090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spot_6-3</th>\n",
       "      <td>0.069513</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013734</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.052950</td>\n",
       "      <td>0.233804</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>0.051433</td>\n",
       "      <td>0.032784</td>\n",
       "      <td>0.018034</td>\n",
       "      <td>0.030266</td>\n",
       "      <td>0.025363</td>\n",
       "      <td>0.014803</td>\n",
       "      <td>0.056242</td>\n",
       "      <td>0.051951</td>\n",
       "      <td>0.026492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spot_7-3</th>\n",
       "      <td>0.056943</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017736</td>\n",
       "      <td>0.012482</td>\n",
       "      <td>0.047838</td>\n",
       "      <td>0.249264</td>\n",
       "      <td>0.034728</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012017</td>\n",
       "      <td>0.047171</td>\n",
       "      <td>0.030835</td>\n",
       "      <td>0.064733</td>\n",
       "      <td>0.029037</td>\n",
       "      <td>0.009194</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.039645</td>\n",
       "      <td>0.060597</td>\n",
       "      <td>0.032436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spot_8-3</th>\n",
       "      <td>0.058560</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.018618</td>\n",
       "      <td>0.073867</td>\n",
       "      <td>0.244776</td>\n",
       "      <td>0.026950</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.053187</td>\n",
       "      <td>0.035445</td>\n",
       "      <td>0.007027</td>\n",
       "      <td>0.041457</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.036121</td>\n",
       "      <td>0.062051</td>\n",
       "      <td>0.051379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spot_9-3</th>\n",
       "      <td>0.045089</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.054261</td>\n",
       "      <td>0.217772</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.005485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021179</td>\n",
       "      <td>0.118834</td>\n",
       "      <td>0.033207</td>\n",
       "      <td>0.054076</td>\n",
       "      <td>0.038668</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.008907</td>\n",
       "      <td>0.038950</td>\n",
       "      <td>0.050761</td>\n",
       "      <td>0.026651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0            Astro  Batch Grouping   CR  Doublet Astro Aqp4 Ex  Doublet Endo  \\\n",
       "spot_1    0.065574        0.000094  0.0                    0.0      0.021691   \n",
       "spot_2    0.088842        0.005988  0.0                    0.0      0.034059   \n",
       "spot_3    0.283756        0.000045  0.0                    0.0      0.000055   \n",
       "spot_4    0.059405        0.000097  0.0                    0.0      0.031219   \n",
       "spot_5    0.060381        0.020950  0.0                    0.0      0.023918   \n",
       "...            ...             ...  ...                    ...           ...   \n",
       "spot_5-3  0.061157        0.000093  0.0                    0.0      0.018658   \n",
       "spot_6-3  0.069513        0.000093  0.0                    0.0      0.013734   \n",
       "spot_7-3  0.056943        0.000094  0.0                    0.0      0.017736   \n",
       "spot_8-3  0.058560        0.000094  0.0                    0.0      0.000972   \n",
       "spot_9-3  0.045089        0.000093  0.0                    0.0      0.004311   \n",
       "\n",
       "0         Doublet VISp L5 NP and L6 CT      Endo  High Intronic   L2/3 IT  \\\n",
       "spot_1                        0.000094  0.046968       0.203390  0.131708   \n",
       "spot_2                        0.000093  0.064074       0.180086  0.072099   \n",
       "spot_3                        0.000045  0.066877       0.090290  0.011668   \n",
       "spot_4                        0.000097  0.046600       0.197762  0.185305   \n",
       "spot_5                        0.000095  0.043542       0.221648  0.111834   \n",
       "...                                ...       ...            ...       ...   \n",
       "spot_5-3                      0.015643  0.055937       0.238554  0.004464   \n",
       "spot_6-3                      0.000093  0.052950       0.233804  0.000093   \n",
       "spot_7-3                      0.012482  0.047838       0.249264  0.034728   \n",
       "spot_8-3                      0.018618  0.073867       0.244776  0.026950   \n",
       "spot_9-3                      0.000093  0.054261       0.217772  0.009787   \n",
       "\n",
       "0               L4  ...        NP     Oligo      Peri     Pvalb       SMC  \\\n",
       "spot_1    0.000094  ...  0.009127  0.045656  0.033060  0.041988  0.034747   \n",
       "spot_2    0.000093  ...  0.014626  0.053172  0.028659  0.018539  0.033710   \n",
       "spot_3    0.000045  ...  0.003812  0.058927  0.028577  0.000045  0.053665   \n",
       "spot_4    0.048333  ...  0.004197  0.053192  0.032850  0.010993  0.031830   \n",
       "spot_5    0.000095  ...  0.014673  0.054637  0.028244  0.018600  0.039960   \n",
       "...            ...  ...       ...       ...       ...       ...       ...   \n",
       "spot_5-3  0.000096  ...  0.017319  0.090831  0.043767  0.007020  0.030524   \n",
       "spot_6-3  0.001807  ...  0.020554  0.051433  0.032784  0.018034  0.030266   \n",
       "spot_7-3  0.000095  ...  0.012017  0.047171  0.030835  0.064733  0.029037   \n",
       "spot_8-3  0.000105  ...  0.003171  0.053187  0.035445  0.007027  0.041457   \n",
       "spot_9-3  0.005485  ...  0.021179  0.118834  0.033207  0.054076  0.038668   \n",
       "\n",
       "0         Serpinf1      Sncg       Sst      VLMC       Vip  \n",
       "spot_1    0.005776  0.016008  0.032795  0.067503  0.054646  \n",
       "spot_2    0.005619  0.081648  0.033538  0.068744  0.065875  \n",
       "spot_3    0.000045  0.005422  0.039513  0.066950  0.061182  \n",
       "spot_4    0.022006  0.012809  0.033963  0.053180  0.043159  \n",
       "spot_5    0.011814  0.004062  0.029430  0.063621  0.068352  \n",
       "...            ...       ...       ...       ...       ...  \n",
       "spot_5-3  0.015671  0.011555  0.023536  0.059534  0.023090  \n",
       "spot_6-3  0.025363  0.014803  0.056242  0.051951  0.026492  \n",
       "spot_7-3  0.009194  0.016414  0.039645  0.060597  0.032436  \n",
       "spot_8-3  0.000593  0.011494  0.036121  0.062051  0.051379  \n",
       "spot_9-3  0.020400  0.008907  0.038950  0.050761  0.026651  \n",
       "\n",
       "[63 rows x 28 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sp_d = OrderedDict()\n",
    "\n",
    "for split in splits:\n",
    "    split_sids = st_sample_id_d[split]\n",
    "    for split_sid in split_sids:\n",
    "        spots = mat_sp_samp_split_d.obs.index[mat_sp_samp_split_d.obs['sample_id'] == split_sid]\n",
    "\n",
    "        pred_sp_d[split_sid] = predictions.loc[spots].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('train',\n",
       "              ['Eng2019_cortex_svz_fov0',\n",
       "               'Eng2019_cortex_svz_fov1',\n",
       "               'Eng2019_cortex_svz_fov4',\n",
       "               'Eng2019_cortex_svz_fov5',\n",
       "               'Eng2019_cortex_svz_fov6']),\n",
       "             ('val', ['Eng2019_cortex_svz_fov2']),\n",
       "             ('test', ['Eng2019_cortex_svz_fov3'])])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_sample_id_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_sc_preds(pred_sp_d, cell_type_index, merged_to_sc):\n",
    "    new_pred_dict = {}\n",
    "    for sid, pred in pred_sp_d.items():\n",
    "        new_pred_dict[sid] = np.empty((pred.shape[0], len(cell_type_index)), dtype=pred.dtype)\n",
    "        for i, merged_cell_type in enumerate(cell_type_index):\n",
    "            if merged_cell_type in merged_to_sc:\n",
    "                old_idxs = [\n",
    "                    sc_sub_dict2[cell_type] for cell_type in merged_to_sc[merged_cell_type]\n",
    "                ]\n",
    "                new_pred_dict[sid][:, i] = pred[:, old_idxs].sum(axis=1)\n",
    "            else:\n",
    "                # no sc cell types map to this st cell type\n",
    "                new_pred_dict[sid][:, i] = 0\n",
    "\n",
    "    return new_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "hue=\"relative_spot_composition\"\n",
    "st_sub_map = get_st_sub_map()\n",
    "cell_type_index = sorted(list(st_sub_map.keys())) + [\"Other\"]\n",
    "\n",
    "# create a mapping from spotless cell types to sc cell types\n",
    "merged_to_sc = {k: [] for k in cell_type_index}\n",
    "\n",
    "for k, v in itertools.chain(\n",
    "    cell_cluster_cell_type_to_spot_composition.items(),\n",
    "    cell_subclass_to_spot_composition.items(),\n",
    "):\n",
    "    if k != \"keep_the_rest\":\n",
    "        if len(v) > 0:\n",
    "            merged_to_sc[\"/\".join(sorted(list(v)))].append(k)\n",
    "        else:\n",
    "            merged_to_sc[\"Other\"].append(k)\n",
    "\n",
    "new_pred_sp_d = _merge_sc_preds(pred_sp_d, cell_type_index, merged_to_sc)\n",
    "\n",
    "st_cell_types_to_sc = {re.sub(\"( |\\/)\", \".\", name): name for name in cell_type_index}\n",
    "\n",
    "ctps = OrderedDict()\n",
    "for split in splits:\n",
    "    ctps[split] = OrderedDict()\n",
    "    for sample_id in st_sample_id_d[split]:\n",
    "        if sample_id not in new_pred_sp_d:\n",
    "            continue\n",
    "        dists_true = (\n",
    "            mat_sp_samp_split_d[mat_sp_samp_split_d.obs[\"sample_id\"] == sample_id]\n",
    "            .obsm[hue]\n",
    "            .rename(columns=st_cell_types_to_sc)\n",
    "            # this adds an \"Other\" column with all 0s\n",
    "            .reindex(columns=cell_type_index, fill_value=0.0)\n",
    "            .to_numpy()\n",
    "        )\n",
    "\n",
    "        mask = np.all(np.isfinite(new_pred_sp_d[sample_id]), axis=1)\n",
    "\n",
    "        y_pred = new_pred_sp_d[sample_id][mask]\n",
    "        y_true = dists_true[mask]\n",
    "\n",
    "        result = np.full((y_true.shape[0],), 1.0)\n",
    "        result[mask] = paired_cosine_distances(y_pred, y_true)\n",
    "\n",
    "        ctps[split][sample_id] = result.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_d[\"mouse_cortex\"] = pd.concat({k: pd.Series(v) for k, v in ctps.items()}).groupby(level=0).mean() #.reindex(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test     0.121183\n",
       "train    0.534351\n",
       "val      0.277304\n",
       "dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_d[\"mouse_cortex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('dlpfc',\n",
       "              train    0.553414\n",
       "              val      0.482188\n",
       "              test     0.497089\n",
       "              dtype: float64),\n",
       "             ('pdac',\n",
       "              train    0.5\n",
       "              dtype: float64),\n",
       "             ('mouse_cortex',\n",
       "              test     0.121183\n",
       "              train    0.534351\n",
       "              val      0.277304\n",
       "              dtype: float64)])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
